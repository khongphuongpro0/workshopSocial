[{"uri":"https://khongphuongpro0.github.io/workshopSocial/4-eventparticipated/4.1-event1/","title":"Event 1: Kick-off OJT Cloud Journey","tags":[],"description":"","content":"Workshop Report \u0026ldquo;RECAP | KICK-OFF AWS FIRST CLOUD JOURNEY WORKFORCE – OJT FALL 2025\u0026rdquo; Event Information Event Name: RECAP | KICK-OFF AWS FIRST CLOUD JOURNEY WORKFORCE – OJT FALL 2025 Date: September 06, 2025 Location: Bitexco Financial Tower, No. 02 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City, Vietnam Role: Participant Event Objectives Mark the official start of the specialized OJT training program in Cloud Computing, AI/ML, and DevOps. Share technology development directions and practical experiences from AWS experts and alumni. Create networking opportunities among students, AWS experts, and partner companies. Key Content Keynote \u0026amp; Industry Sharing Technology Trends: Sharing the development direction of AWS Vietnam, emphasizing the role of Cloud, AI, and DevOps in the digital age. The Importance of Cloud: Clear realization of the necessity of \u0026ldquo;going cloud\u0026rdquo; for modern applications and career development prospects. Specialized Areas: Overview of the knowledge domains that will be covered: Cloud Computing, AI/ML, DevOps, Security, Data \u0026amp; Analytics. Alumni \u0026amp; Career Sharing Success Stories: Alumni from the First Cloud Journey program provided inspiration through real-life stories about their career paths. Role Transition: Sharing on the skills and mindset needed to adapt to new technology roles, from Cloud Engineer to GenAI Engineer. Networking \u0026amp; Q\u0026amp;A Bridge to Knowledge: The event served as an important bridge between academic knowledge, practical technology, and career opportunities. Direct Exchange: Opportunities to network and ask questions directly to AWS experts and the technology community. Key Takeaways Mindset and Career Orientation Specialized Direction: Clearly define the learning and working path in the Cloud/DevOps/AI/ML fields. Alumni Perspective: Gaining inspiration from the real-world experiences of predecessors, which strengthens the resolve to pursue a career in Cloud. Networking Skill: Capitalizing on the opportunity to connect and expand professional relationships within the technology expert community. General Knowledge The Role of DevOps/AI: Understanding the integration and crucial role of DevOps and Artificial Intelligence in modern projects. Value of OJT: Recognizing the importance of the On-the-Job Training (OJT) program in transferring in-depth knowledge and practical work experience. Application to Work Setting OJT Goals: Establishing clearer learning objectives, focusing on areas like DevOps (ECS/EKS) and Security (WAF/Security Group) for application in the internship project. Communication and Connection: Proactively seeking support and exchanging experiences with members of the OJT community and mentors. Event Experience Vibrant Atmosphere: The Kick-off event had a very vibrant and inspiring atmosphere, creating strong motivation for the upcoming learning journey. Real-world Insights: Listening to shares from experts and alumni provided a practical outlook, distinct from theoretical learning. The Kick-off opened up a promising journey of learning, building, and development, helping me prepare best to become a part of the professional Cloud workforce.\n"},{"uri":"https://khongphuongpro0.github.io/workshopSocial/3-blogstranslated/3.1-blog1/","title":"Blog 1.Extending SAP Field Service Management with AWS: A Clean Core Approach for Attachment Storage","tags":[],"description":"","content":"Digital transformation in field service operations has led to an exponential increase in assets generation and therefore storage requirements. Organizations using SAP Field Service Management (SAP FSM) face growing challenges in managing digital assets captured by field service technicians. These assets can consist of equipment pictures, forms, customer signatures and other critical assets managed when in the field. This post demonstrates how to leverage Amazon Web Services (AWS) to create a scalable, cost-effective attachment storage solution for SAP FSM while adhering to SAP Clean Core Extensibility principle.\nOverview Field service operations generate substantial amounts of attachments throughout their daily activities. While SAP FSM provides native storage capabilities, organizations often require more storage space and a more flexible and cost-effective solutions that allow them to maintain data ownership for compliance reasons while enabling integration with other business processes. By integrating SAP FSM with an organization’s AWS infrastructure, this solution empowers enterprises to maintain control over their data, while also enabling advanced processing and analysis to extract maximum value from the assets created in the field.\nThe integration proposed in this blog allows organizations to store the attachments generated in SAP FSM in an Amazon Simple Storage Service (S3) bucket in one of their own AWS Accounts, enforcing data ownership and reducing the data footprint in SAP FSM. In addition, an organization can implement additional data processing and analysis on the stored attachments by leveraging AWS services such as Amazon Textract for text extraction, Amazon Rekognition for image analysis, or Amazon Comprehend for natural language processing. Having the full control of these assets is also the starting point to power more complex solution by leveraging Large Language Models (LLMs) and Knowledge Base integrations.\nHow it works The process begins in the field, where a service technician completes their work at a customer site. Using their mobile device with the SAP FSM application, they capture crucial documents. When they tap “upload” in their FSM application, they set in motion a chain of events designed to ensure the documents are stored, safely accessible within the organization.\nAs the attachment lands in SAP FSM, the system’s business rule engine immediately detects this new content. The engine, configured with specific rules for attachment handling, springs into action. The business rule extracts essential information about the attachment (unique identifier, filename, description, timestamp, and relationship to the service call).\nThe business rule then makes an HTTPS request with the attachment’s metadata to a dedicated API endpoint in the customer’s AWS environment. This API, built on Amazon API Gateway, is protected with a unique API key for the SAP FSM tenant, and IP allowlisted to the SAP FSM environment.\nUpon receiving the metadata, the API publishes it to Amazon EventBridge (EventBridge). The handling of this event happens asynchronously by an AWS Lambda (Lambda) function – the attachment processor. This function performs several critical tasks in sequence:\nIt authenticates with SAP FSM using OAuth2 credentials stored in AWS Secrets Manager. It retrieves the actual attachment content using SAP FSM’s attachment API. It processes this content, enriches it with the original metadata, and stores everything in a designated S3 bucket. Here the detailed architecture diagram:\nSAP FSM Extension The solution allows FSM customers to view operational metrics about the Amazon S3 back-ups of the attachments, and can be installed as an SAP FSM extension.\nThe extension is a web page, built with React, and hosted on Amazon CloudFront and Amazon S3. The extension provides real-time visibility into your S3 storage metrics via a built-in Amazon CloudWatch dashboard.\nExtension Authentication Because the extension runs inside user’s browsers (as an HTML iframe), it uses a new authentication pattern:\nThe extension uses the SAP FSM SDK to acquire a short-lived token (JWT) for the user, which is cryptographically signed by SAP FSM. The token is then used to authorize requests from the extension to AWS API Gateway. The solution includes a Lambda Custom Authorizer that uses the library aws-jwt-verify to verify the token, checking the tenant details and the cryptographic signature. Getting Started The high level steps of the implementation includes:\nReview the prerequisites and ensure you have the necessary AWS and SAP FSM access. Deploy the solution using the provided CloudFormation templates in your region. Configure the OAuth2 credentials in SAP FSM. Install and configure the extension in SAP FSM Extension marketplace. Monitor the implementation through the provided CloudWatch dashboard. Cost Overview The costs related to deploying and using the solution are mainly related to the volume of attachments created in FSM. For testing purposes, the costs are minimal, less than $1 per month.\nProduction Cost Assumptions Assumption Value Details Number of attachments created in SAP FSM (FSM agent) 100,000 per Month Volume of attachment (GB) 100 per Month Number of http requests for Extension UI (FSM admin user) 10,000 per Month AWS Region eu-central-1 (Frankfurt) Currency USD US Dollars (Table 1: Assumptions for cost calculation)\nCost Breakdown Total Monthly Cost: 6.81 USD Total Yearly Cost: 81.72 USD Description AWS Service Monthly (USD) Year (USD) Distribution Amazon CloudFront 0.23 2.76 S3 Bucket (attachments) S3 Standard 3.04 36.48 REST API Gateway Amazon API Gateway 0.37 4.44 Metrics Amazon CloudWatch 3.01 36.12 Other Serverless services (Lambda, EventBridge, Data Transfer) AWS Serverless ~0.16 ~1.92 (Table 2: Cost details summary)\nConclusion The integration between SAP Field Service Management and AWS demonstrates how organizations can leverage cloud-native serverless services to build scalable, secure, and cost-effective solutions while maintaining a clean core approach.\nKey Benefits: Reduced data footprint in SAP FSM. Enhanced data sovereignty and control. Cost-effective scalable storage through S3. Built-in monitoring and observability. Potential for advanced analytics and AI/ML integration. Seamless user experience for field technicians. The solution’s serverless architecture ensures minimal operational overhead.\nJoin the SAP on AWS Discussion AWS provides public question and answer forums on our re:Post Site. Refer to the AWS Serverless blog section to learn more about serverless and event driven architecture patterns.\n"},{"uri":"https://khongphuongpro0.github.io/workshopSocial/3-blogstranslated/3.3-blog3/","title":"Blog 3. Build SAP applications faster with Amazon Q Developer","tags":[],"description":"","content":"Introduction All companies are looking for ways to help their developers improve productivity, build applications faster, and simplify the burden of maintaining legacy code. Amazon Q Developer is a generative AI tool that can help companies eliminate the technical debt associated with their highly customized SAP environments and deliver new features faster. In this blog, we will discuss how you can use Amazon Q Developer to help your SAP developers improve productivity and innovate faster.\nSAP is a mission critical application that powers business operations for thousands of companies globally. Over the years (and decades), many customers have customized SAP to meet their company’s unique requirements. To create these customizations, customers have utilized SAP’s ABAP programming language to write purpose-built programs that deliver required business functionality.\nWhile ABAP programs have helped companies make SAP work for their business, this has led to highly customized SAP environments that are difficult to operate and upgrade. We commonly hear about complex ABAP code written “decades ago” that lacks documentation or the original developers have retired. Now, as these companies look to migrate to the cloud, implement SAP’s latest offerings including S/4HANA, and adopt SAP’s recommended “clean core” strategy, legacy code presents many challenges.\nSimplifying SAP modernization with Amazon Q Developer Amazon Q Developer can help companies overcome the challenges of legacy code, enabling faster, lower cost SAP upgrades. This, in turn, simplifies regulatory compliance, security patching, and benefitting from new software capabilities. Amazon Q Developer is capable of generating documentation, both functional and technical specifications, for legacy ABAP code, saving hours of valuable time.\nAmazon Q Developer works across the entire SAP suite of programming frameworks, including classical SAP ABAP, SAP ABAP RESTful Application Programming Model (RAP), and SAP Cloud Application Programming Model (CAP). Q Developer is available as an IDE extension within VS Code, Eclipse, and several more. The Eclipse version of Amazon Q developer will be fully functional for all the different ABAP object types in the near future.\nCustomers using Q Developer with other programming languages (Java, Python) have reported up to 40% increase in developer productivity and up to 80% acceleration in various development tasks. We are already starting to hear from SAP customers (and partners) who are realizing similar benefits for ABAP development.\nSaul Dave, Senior Director of Enterprise Systems, at Zappos.com stated that Amazon Q Developer will be a game-changer for our ABAP development and application support teams.\nNow, we will dive into four example use cases to show how Q Developer can improve productivity for SAP developers:\nGenerating ABAP code Generating BTP and Fiori applications Generating test cases Documenting legacy ABAP code Generating ABAP Code Amazon Q Developer can interpret natural language prompts to create functional code. In this example, ABAP code is generated to display open sales orders, which includes capability to filter on order number and customer number. The developer creates the code by entering the following prompt in Q Developer:\n“Generate an ABAP report named zhprp_sales_order_overview, showing list of open sales orders, filter either by order number or customer number (sold-to-party). Include: Sales order number, Sold-to-party, Order Creation date, Line Item number, Material Number, Ordered quantity, Confirmed Quantity. Order the records by sales order number. Display the output in ALV format.”\nCode Generation for Fiori and BTP The next example shows how to develop an entire Fiori application using Q Developer. This example uses a single prompt that steps through the process to create the front end and back end components, including a CDS view, an OData interface, and a UI. The prompt used is as follows:\n“Provide me with all the things I need to do to create a fiori application for the sales order(create, update, delete) and then you can handhold me while I am creating each step. In addition to that I want to have a class to insert dummy data and test classes for my cds view for TDD.”\nAmazon Q follows a layered approach:\nDatabase layer: Where necessary table structures are created. CDS layer: Where a root CDS view is established to provide a business-oriented view of the data. Business layer: Amazon Q helps generate the behavior definition of the CDS view, including behavior implementation and test classes. Service layer: Involves creating service definition and binding for OData V2 exposure. UI layer: Amazon Q assists with UI annotations using metadata extension. The final steps involve creating custom controller actions and HTML UI5 components to generate the complete Fiori application.\nUnit Test Case Generation Amazon Q developer helps create test classes for existing code when documentation and original developers are unavailable. Users simply paste their code into Q’s inline chat, which then analyzes and generates comprehensive test scenarios automatically. Any syntax errors in generated code can be quickly fixed through Q’s inline chat feature with one-click implementation. The generated test class is immediately available in the SAP system and can be fine-tuned as needed.\n“Generate unit test class for public methods ”Provide the your class logic/details here”\nThis feature allows developers to test the business logic easily even after multiple iterations, saving a huge amount of effort on manual testing.\nDocumenting legacy ABAP code: The following example showcases how Amazon Q Developer analyzes ABAP code and automatically generates documentation, adapting to both existing codebases and newly created code based on your custom templates in the chat window. You can easily convert the documentation to PDF or Word documents. Amazon Q Developer streamlines the documentation process by extracting key information and maintaining consistent formatting standards. For this example, the following prompt was used:\n“Generate a technical documentation of the above ABAP code. Make sure to provide highly detailed documentation, clearly explaining the action performed each of the components using following pointers as template:\nClass/Program name Class/Program Overview Technical Specifications 3.1 Data Structure 3.2 Selection Screen (If provided) Main Components 4.1 Subroutines/Methods Test Implementation (If provided) 5.1 Test Methods 5.2 Test Setup Technical Dependencies Error Handling Performance Considerations” This feature allows organizations to easily understand and document the business processes impacted by relevant custom objects, helping them during migration and knowledge transfer.\nAs you can see from the examples above, Amazon Q Developer offers powerful capabilities to reduce manual work for SAP developers, helping customer more rapidly modernize their business processes. We are excited to see how customers will continue to take advantage of these capabilities.\nPricing Models: You can get started on the Amazon Q Developer Free Tier, which offers 50 chat interactions per month, software development assistance 5 times per month, and transformation of up to 1,000 lines of code per month. The Pro Tier offers all features in the free tier, plus enterprise access control features to manage users and policies, ability to customize Q Developer to your code base to improve suggestions, and higher usage limits for advanced features. Click here to explore our detailed pricing plans.\nModernize your legacy SAP code today. Access this [Workshop] for step-by-step instructions on setting up Amazon Q Developer. Watch for upcoming YouTube videos that demonstrate SAP use cases and provide deep dives into these and other scenarios. To learn more about Amazon Q Developer, view our [documentation] or reach out to our teams to discuss how we can help modernize your legacy SAP code.\n"},{"uri":"https://khongphuongpro0.github.io/workshopSocial/3-blogstranslated/3.2-blog2/","title":"Blog 2. Building Enterprise-Ready Hybrid Network Connectivity on AWS for SAP Cloud ERP Private (formerly known as RISE with SAP)","tags":[],"description":"","content":"Introduction Are you ready to unlock the full potential of your SAP workloads on AWS? Let’s solve one of the most crucial pieces of the puzzle: establishing secure, reliable network connectivity between your company networks and your cloud ERP workloads.\nAt AWS, as we’ve helped customers implement their SAP Cloud ERP Private workloads (formerly known as RISE with SAP), three questions understandably emerge:\n“How do we establish secure connections to our private cloud ERP environments?” “What’s the most cost-effective network architecture for our use case?” “Should we implement Direct Connect, Site-to-Site VPN, or both?” Network connectivity decisions made today will impact your SAP operations for years to come, affecting everything from system performance to disaster recovery capabilities. In this guide, we’ll cut through the complexity and show you how to connect your existing infrastructure to AWS for SAP Cloud ERP Private.\nGetting Started: Understand the Shared Responsibility Model When implementing workloads for SAP Cloud ERP Private, the responsibilities are split:\nSAP manages the AWS environment where Cloud ERP Private operates. You manage the network connectivity between your infrastructure and the SAP Cloud ERP Private environment in AWS. This division means you need a clear connectivity strategy before your implementation begins.\nLet’s Meet Your Business Needs Every organization has unique requirements. We see these three starting points:\nFocused Implementation: Prioritizes simplicity and security for rapid deployment. Existing AWS Infrastructure: Integrates SAP Cloud ERP Private into an existing network architecture. Multi-Region Operations: Requires sophisticated, multi-region networking with enhanced control and automation. What This Post Covers We’ll walk through three connectivity architectures that align with different business requirements:\nFoundation Architecture: A streamlined, secure solution ideal for rapid deployment. Integrated Architecture: A hybrid approach that optimizes existing AWS investments and provides automated failover. Comprehensive Architecture: An enterprise landing zone approach for complex, multi-region deployments with advanced automation. Option 1: Building Mission-Critical Connectivity with AWS Direct Connect When your SAP workloads demand consistent, high-performance connectivity, AWS Direct Connect (DX) delivers. This solution provides dedicated, private network connections between your infrastructure and SAP Cloud ERP Private on AWS.\nWhy Choose Direct Connect? For mission-critical SAP environments, DX offers:\nConsistent low-latency performance Predictable network behavior Dedicated bandwidth Enhanced security through private connectivity Consider DX when you need: Production SAP environments requiring consistent low-latency performance. Regular large-volume data transfers (2+ TB daily). Predictable response times for mission-critical operations. Choosing Your Direct Connection Option: Hosted Connections: Fast deployment through AWS Direct Connect Partners. Ideal for most SAP Cloud ERP Private deployments. Dedicated Connections: Maximum control, custom bandwidth up to 100 Gbps. Recommended for high-volume, latency-sensitive workloads. Important Security Note: Neither connection type includes built-in encryption. Consider implementing additional security measures such as MacSec for enhanced protection.\nBuilding Resilient Connectivity For mission-critical workloads, implement multiple DX connections for high availability:\nUse the AWS Direct Connect Resiliency Recommendations to choose your optimal model. Implement the AWS Direct Connect Resiliency Toolkit for redundant connections. Option 2: Optimize Cost and Reliability with Direct Connect + VPN Failover By combining AWS Direct Connect with Site-to-Site VPN, you can create a resilient network architecture that balances performance with cost-effectiveness.\nBuilding Your Hybrid Connection Strategy Direct Connect serves as the primary path (consistent performance). AWS Site-to-Site VPN stands ready as an automatic failover option, providing encrypted connectivity through the internet. Getting Started with Site-to-Site VPN The key advantage of incorporating AWS Site-to-Site VPN is its rapid deployment capability (within days).\nThe VPN connection provides:\nBuilt-in IPSec encryption for secure data transfer. Flexible bandwidth based on your internet connection. Pay-as-you-go pricing model. Making the Right Choice for Your Business This hybrid approach works particularly well for organizations that need to:\nBalance performance requirements with budget constraints. Support remote offices with varying bandwidth needs. Establish disaster recovery capabilities. Option 3: Building an Enterprise Foundation with AWS Landing Zone When your journey to SAP Cloud ERP Private is part of a broader cloud strategy, implementing an AWS Landing Zone creates a foundation that grows with your business.\nWhy Consider a Landing Zone Approach? A landing zone is a well-architected, multi-account AWS environment that follows best practices. It provides:\nCentralized security controls and monitoring. Standardized network architecture. Consistent governance across regions. Flexible integration options. The Landing Zone Accelerator (LZA) helps you implement this foundation quickly and securely.\nCreating Your Connected Environment Within your landing zone, AWS Transit Gateway serves as a central hub for network traffic, allowing you to:\nConnect multiple VPCs. Integrate on-premises networks. Implement consistent security policies. Monitor traffic patterns centrally. Scale connectivity as needed. Real-World Applications Organizations implement a landing zone approach when they:\nNeed to maintain strict security and compliance standards. Plan to expand beyond core SAP workloads (e.g., IoT, analytics). Operate across multiple geographic regions. Bringing It All Together: Building Your Optimal Network Strategy AWS offers flexible connectivity options that can be combined to match your specific needs:\nCombination Key Benefits DX + VPN (Options 1 + 2) Performance, Resilience, Cost-effective global reach. Landing Zone + DX (Options 3 + 1) Maximum control, Highest availability, Future-ready foundation. Landing Zone + Hybrid (Options 3 + 2) Scalable architecture, Smart cost management, Automated failover. Complete Enterprise Solution (Options 1 + 2 + 3) Maximum flexibility, Full redundancy, Global reach. Taking Action: Your Next Steps Assess Your Requirements: Map current network requirements, document performance needs, and consider future growth. Plan Your Approach: Select your connectivity strategy, define implementation phases, and align stakeholders. Prepare for Implementation: Create technical requirements, engage AWS early, and develop testing plans. Conclusion: Your journey to successful AWS for SAP Cloud ERP Private connectivity begins with a single step. Contact your AWS account team or open a case in the AWS Support Portal to start building your optimal network architecture.\nTài nguyên bổ sung (Additional Resources): Guidance for Building an Enterprise-Ready Network Foundation for RISE with SAP on AWS AWS Direct Connect + AWS Transit Gateway + AWS Site-to-Site VPN Connecting to RISE with SAP from on-premises networks "},{"uri":"https://khongphuongpro0.github.io/workshopSocial/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nguyen Hai Minh Phuong\nPhone Number: 0837309709\nEmail: phongphuong22240@gmail.com\nUniversity: Saigon University\nMajor: Information Technology\nClass: DCT1219\nInternship Company: Amazon Web Services Vietnam Company Limited\nInternship Position: FCJ Cloud Intern\nInternship Period: From August 12, 2025 to November 12, 2025\nReport Contents Worklog Proposal Translated Blogs Participated Events Workshop Self-evaluation Feedback and Contributions "},{"uri":"https://khongphuongpro0.github.io/workshopSocial/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get to know the members of First Cloud Journey. Participate in the Kick-Off of FCJ 2025. Tasks to be carried out this week: Day Tasks Start Date End Date 2 - 6 - Get acquainted with FCJ members - Get to know Mr. Kevin, be briefed on the internship process, prepare to join the first event - Read and note the rules and regulations at the internship unit 09/01/2025 09/05/2025 7 - Join the First Cloud Journey 2025 Kick-Off - Meet the FCJ mentors, receive guidance, and listen to shares from Mr. Hung and other mentors. - Take an introduction photo with the SGU team members 09/06/2025 09/06/2025 Week 1 Achievements: Met and interacted with FCJ members and mentors. Read and understood the rules for participating in the internship. Gained an understanding of the tasks and responsibilities during the internship period. "},{"uri":"https://khongphuongpro0.github.io/workshopSocial/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Learn about AWS services. Learn how to complete the final workshop. Create an AWS Account. Tasks to be carried out this week: Day Tasks Start Date End Date Resources 2 - Self-study AWS services, read documentation to get an overview. 09/08/2025 09/08/2025 https://www.udemy.com/course/vo-long-ve-amazon-web-services/learn/lecture/47226265#overview\nhttps://www.youtube.com/watch?v=HxYZAK1coOI\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=4 (Module 1 - 7) 3 - Create an AWS Account, continue setting up and learning about AWS overview. 09/09/2025 09/09/2025 https://www.youtube.com/watch?v=waR5S_lljrk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=11\nhttps://www.youtube.com/watch?v=1dG5xutGbr4\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=12 4 - 6 - Learn how to draw AWS architecture - Review how to do a WorkShop - Learn more about Groups, Roles, Policies - Learn about cost optimization 09/10/2025 09/11/2025 https://www.udemy.com/course/vo-long-ve-amazon-web-services/learn/lecture/47226303#overview\nhttps://www.youtube.com/watch?v=IY61YlmXQe8\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=9 Week 2 Achievements: Understood what AWS is, learned about its usage, cost optimization, and the purposes of AWS services.\nSuccessfully created and configured an AWS Account, set up MFA for the account.\nLearned how to draw AWS architecture to prepare for project architecture design.\nLearned to create tools and methods for completing the workshop.\n"},{"uri":"https://khongphuongpro0.github.io/workshopSocial/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Brainstorm ideas for the upcoming project. Research technologies for project implementation. Continue learning about AWS services and practice with basic labs. Tasks to be carried out this week: Day Tasks Start Date End Date Resources 2 - Learn about IAM, VPC, Policies services Practice: + Create a VPC + Create VPC, create basic Policies 09/15/2025 09/15/2025 https://www.udemy.com/course/vo-long-ve-amazon-web-services/learn/lecture/47226303#overview\nhttps://000003.awsstudygroup.com/vi/ 3 - Learn about EC2, Security Group, Subnet and Route Table, Internet Gateway\n- Practice:\n+ Basic creation, learn about EC2 types + Practice creating an EC2 instance combined with VPC, Subnet, Security Group, Route Table and basic Internet Gateway for user connection 09/16/2025 09/16/2025 https://cloudjourney.awsstudygroup.com/ https://000004.awsstudygroup.com/vi/\nhttps://www.udemy.com/course/vo-long-ve-amazon-web-services/learn/lecture/47226313#overview 4 - 6 - Research and decide on a project for the workshop - Design the basic functions of the project - Design the project\u0026rsquo;s ERD - Research technologies and AWS services to select for the project. 09/17/2025 09/19/2025 Week 3 Achievements: Gained understanding of core AWS services:\nIAM VPC EC2 Security Group Selected a Social Media project for the Workshop.\nDesigned an overview ERD to facilitate database design.\n"},{"uri":"https://khongphuongpro0.github.io/workshopSocial/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Continue researching and selecting technologies for project implementation. Continue learning about AWS services. Finalize the database design. Tasks to be carried out this week: Day Tasks Start Date End Date Resources 2 - Research and select technologies for the Workshop - Set up and configure Visual Studio 2022 for the project with .Net Backend and Reactjs Frontend 09/22/2025 09/23/2025 3 - Learn about RDS, ECS, S3, and Route 53 services Practice: + Practice with S3, learn how to store data, files\u0026hellip; 09/23/2025 09/23/2025 https://www.udemy.com/course/vo-long-ve-amazon-web-services/learn/lecture/47226313#overview https://www.udemy.com/course/vo-long-ve-amazon-web-services/learn/lecture/47226319#overview 4 - 5 - Continue learning about other basic Storage: EBS, EFS Practice:\n+ Practice SSH connection to EC2, experiment with attaching Elastic IP to EC2, trial hosting Apache on EC2 09/24/2025 09/25/2025 https://www.udemy.com/course/vo-long-ve-amazon-web-services/learn/lecture/47226319#overview https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/26098410#overview (31 - 55) 6 - Select technologies, download packages, push code to GitHub: - Convert ERD into a database, perform basic connection and migration with the project 09/26/2025 09/28/2025 Week 4 Achievements: Gained a better understanding of EC2 types. Learned more about IP types. Learned to configure Subnet and Internet Gateway to host a server on EC2. Converted the database from ERD format to MySQL data structure. Selected software and technologies for the project, including: IDE: Visual Studio 2022 Web Technologies: ++ Backend: .Net, JWT, SendGrid, WebSocket, Kafka. ++ Frontend: ReactJS, Bootstrap. ++ Database: MySQL Cloud (AWS): ++ IAM ++ ECS ++ S3 ++ CloudFront ++ RDS ++ VPC ++ ASG (Auto Scaling Groups) ++ ALB (Application Load Balancer) \u0026hellip;\u0026hellip; "},{"uri":"https://khongphuongpro0.github.io/workshopSocial/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Design code architecture. Create Layers and basic classes for project deployment. Learn more about AWS services, deepen understanding of RDS, Auto Scaling Group, and Amazon Load Balancer. Tasks to be carried out this week: Day Tasks Start Date End Date Resources 2 - Continue learning more about RDS, EC2, Security Group, Auto Scaling Group, Amazon Load Balancer Practice: + Practice creating RDS and linking it with EC2 to create a basic test database 09/29/2025 09/29/2025 https://000005.awsstudygroup.com/vi/4-create-rds/ 3 - 6 - Implement Onion architecture\n- Design Layers\n- Design classes such as Service, Repository, IRepository Practice:\n+ Use AI to speed up the design of IBaseRepository and BaseRepository classes for common data retrieval code from the database\n+ Create specific code for each Repository with unique features for all remaining Repository classes corresponding to tables such as Account, Address, BlockedUser, Comment, CommentLike, CommentMember, ConversationNickName, Conversation, District, Follow, Friend, User, UserToken, MessageAttachment, MessageReaction, MessageRead.\n+ Design DTOs to return data for the Repositories 09/30/2025 10/05/2025 DeepSeek, Claude Code Week 5 Achievements: Gained deeper understanding of EC2, RDS, ASG, and ALB. Designed Onion architecture for the project. Successfully designed specific classes for database access: IRepository, Repository. "},{"uri":"https://khongphuongpro0.github.io/workshopSocial/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Design the remaining Repository classes Build some basic service classes and controllers to test APIs Tasks to be carried out this week: Day Tasks Start Date End Date Resources 2 - 5 Practice: + Design the remaining Repository classes: Notification, PostLike, Post, Province, Role, Share, Sticker, Ward + Additional design: templates for conversations, Pages, and social media groups + Add corresponding Repositories: ConversationTemplate, Group, GroupMember, Page, PageFollow 10/06/2025 10/10/2025 6 Practice: + Build Conversation Service class + Build Controller class to test API with PostMan + Research and build SendGrid Service for sending emails. 10/11/2025 10/13/2025 Week 6 Achievements: Completed all Repository classes Completed Conversation Service for creating both group and individual conversations Built Controller and successfully tested some Service functions Built Service using SendGrid for sending emails "},{"uri":"https://khongphuongpro0.github.io/workshopSocial/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Configure AWS services to prepare for project deployment to the cloud. Complete several Services and Controllers for basic functions such as sending messages, creating conversations, adding friends, blocking users\u0026hellip; Tasks to be carried out this week: Day Tasks Start Date End Date Resources 2 Practice: + Configure and design EC2, VPC, create both public and private subnets to prepare for the project + Continue designing the Conversation service 10/13/2025 10/13/2025 3 - 6 Practice: + Complete the design of Conversation Service and Conversation Controller, test with PostMan + Design and build Message Service and Controller for sending messages, files,\u0026hellip; 10/13/2025 10/19/2025 Week 7 Achievements: Completed building the Conversation Service. Designed and built the Service for sending messages. Completed API testing with PostMan for the implemented functions. "},{"uri":"https://khongphuongpro0.github.io/workshopSocial/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Continue setting up and building additional missing features such as JWT. Complete the message sending functionality for both group and individual conversations. Complete User functions such as login, registration, confirmation email sending, and password recovery. Tasks to be carried out this week: Day Tasks Start Date End Date Resources 2 - 4 Practice: + Complete messaging functionality, test Controller for messaging\n+ Continue working on Service for friend adding, blocking, following 10/20/2025 10/22/2025 5 Practice: + Integrate JWT and BCrypt for password encryption + Implement login and registration + Design RDS to prepare database on cloud 10/23/2025 10/24/2025 6 Practice: + Continue completing login and registration Service functions, add password recovery and change password functions, integrate email sending with SendGrid 10/24/2025 10/26/2025 Week 8 Achievements: Completed Service and Controller classes for the following functions: Send messages, stickers, files, images, videos Edit messages Delete messages Login Registration Send email for password recovery Send email for account confirmation Successfully tested corresponding functions with PostMan "},{"uri":"https://khongphuongpro0.github.io/workshopSocial/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Complete assigned blog translation ahead of schedule. Continue completing other Services such as social media, posting, and adding features to Conversation Service like conversation templates. Tasks to be carried out this week: Day Tasks Start Date End Date Resources 2 - Translate 3 assigned blogs - Build SAP applications faster with Amazon Q Developer - Building Enterprise-Ready Hybrid Network Connectivity on AWS for SAP Cloud ERP Private (formerly known as RISE with SAP)\n- Extending SAP Field Service Management with AWS: A Clean Core Approach for Attachment Storage 10/27/2025 10/27/2025 3 - Review the 3 translated blogs and report on assigned tasks 10/28/2025 10/28/2025 4 Practice: + Continue completing social media feature Services: Get friend list, get online friends list, get friend request list. 10/29/2025 10/30/2025 5 Practice: + Complete Services for retrieving lists, continue with getting member lists in social media groups,.. continue building Controllers to test APIs for completed functions 10/30/2025 10/30/2025 6 Practice: + Create Services for social media features such as posting, joining social media groups, creating fanpages, post-related functions 10/31/2025 11/02/2025 Week 9 Achievements: Completed Service and Controller for testing chat-related functions. Completed assigned blog translation. Completed part of Service and Controller related to post features. "},{"uri":"https://khongphuongpro0.github.io/workshopSocial/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Typically, and also as standard, a worklog is maintained over about 3 months (throughout the internship period) with the weekly content as follows:\nWeek 1: Getting started with the internship, receiving guidance, joining the group\nWeek 2: Learning about AWS services\nWeek 3: Continuing with AWS services, proposing a project\nWeek 4: Starting the Social Media project, continuing to learn about AWS services and designing the database\nWeek 5: Designing code architecture, creating classes, learning about AWS services\nWeek 6: Continuing to design Repository classes.\nWeek 7: Designing and completing basic service and controller functionalities\nWeek 8: Continuing the project, integrating JWT and SendGrid\nWeek 9: Translating Blogs, continuing to complete remaining Service classes\nWeek 10: Adding more features, completing core social media functionalities\nWeek 11: Finalizing remaining features, implementing ReactJS for the frontend\nWeek 12: ReactJS Integration and Full System Deployment to AWS (ECS, S3, RDS)\n"},{"uri":"https://khongphuongpro0.github.io/workshopSocial/5-workshop/5.3-s3-vpc/","title":"Route 53 and VPC Network Configuration","tags":[],"description":"","content":"This section covers domain registration, SSL/TLS certificate configuration, and setting up the Virtual Private Cloud (VPC) for backend resources.\n3.1. Route 53 and ACM (SSL Certificate) Configuration 3.1.1. Domain Registration Go to the Route 53 service and select Register domain. Enter the desired domain name and choose the domain extension type (e.g., select .click for affordability). Click Select and proceed to checkout. Enter contact information and click Next. Review the information and click Submit to finalize the purchase. The newly purchased domain appears in the list. 3.1.2. Create SSL Certificate using ACM Go to the ACM (AWS Certificate Manager) service, and select Request a certificate. Click Next (or keep the default Public certificate). Select the recently purchased domain and click Request. Note: two domain names are needed here (e.g., social-sgu-media.click and *.social-sgu-media.click). After the request, create CNAME records to validate the domain through Route 53. 3.1.3. Update Certificate and Domain for CloudFront Go back to the CloudFront Distribution, select the Distribution, and go to Settings (or General), then select Edit. Enter the purchased domain in the Alternate domain names (CNAMEs) field. Select the SSL certificate (Custom SSL certificate) created using ACM and click Next. Finally, click Add domains (or Save changes). 3.1.4. Create Route 53 Record pointing to CDN In Route 53, go to Hosted zones, select the purchased domain, and click Create record. Create the record: select Type A, choose Alias pointing to the CloudFront Distribution, and click Create records. Routing is successful. Verify by accessing the domain in the browser. 3.2. VPC (Virtual Private Cloud) Setup 3.2.1. Create VPC Go to the VPC service and click Create VPC. Set a name and choose an IP range (e.g., 10.0.0.0/16). Click Create. VPC creation is successful. 3.2.2. Create and Attach Internet Gateway Create an Internet Gateway (IGW). Creation is successful. Attach it to the newly created VPC (Attach to VPC). Attachment is successful. 3.2.3. Create Subnets We create 4 Subnets: 2 Public Subnets (for ALB/NAT Gateway) and 2 Private Subnets (1 for ECS, 1 for RDS), each in 2 different Availability Zones to ensure high availability.\nGo to the Subnets tab and click Create subnet. Private Subnet for RDS (e.g., Private-RDS-A): Set a name, select the VPC, select the Availability Zone (AZ), and create an IP range (e.g., 10.0.10.0/24). Click Create. Similarly, create the remaining 3 Subnets: Private-RDS-B (Different AZ, different IP range) Private-ECS-A (Different AZ, different IP range) Public-A (Different AZ, different IP range) Public-B (Different AZ, different IP range) The created Subnets are now available. "},{"uri":"https://khongphuongpro0.github.io/workshopSocial/4-eventparticipated/4.2-event2/","title":"Event 2: AWS Perimeter Protection Workshop","tags":[],"description":"","content":"Workshop Report \u0026ldquo;Secure Your Applications: AWS Perimeter Protection Workshop\u0026rdquo; Event Information Event Name: Secure Your Applications: AWS Perimeter Protection Workshop | Ho Chi Minh City Date \u0026amp; Time: 09:00 - 17:00 on November 19, 2025 Location: 26th Floor (Room 26.100), Bitexco Financial Tower, No. 02 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City, Vietnam Role: Participant Level: Hands-on workshop (Level 200, 300) Event Objectives Provide practical skills to protect web applications from escalating threats (such as DDoS, application-layer attacks). Equip participants with proven AWS security frameworks to build a strong Perimeter Protection architecture. Featured Speakers Nguyễn Gia Hưng - Head of Solutions Architect - Vietnam \u0026amp; Cambodia Julian Ju - Senior Edge Services Specialist Solutions Architect Kevin Lim - Senior Edge Services Specialist GTM Key Content 1. From Edge to Origin: CloudFront as Your Foundation CDN Architecture: Learn how to design and optimize global content distribution with Amazon CloudFront. Core Capabilities: The key capabilities of the CDN, from edge locations to security controls, building a solid foundation for serving content at scale. 2. Attack Surface Defense: WAF \u0026amp; Application Protection WAF Security: Protect web applications from threats by using AWS WAF to block malicious traffic. OWASP Top 10 Mitigation: Learn how to mitigate common vulnerabilities, including defending against bot attacks. 3. Hands-on Workshop Optimize Internet Web Application: Practical exercise in optimizing web applications through AWS Edge services. Secure Internet Web Application: Practical security setup exercise, discussing real-world implementation scenarios. Quiz Activity: A Q\u0026amp;A game at the end of the Workshop to reinforce knowledge and offer a chance to receive gifts from AWS. Key Takeaways Security Techniques Perimeter Security: Clearly understand the role of Edge services (CloudFront, WAF) in protecting the application at the outermost layer. Optimization and Defense: Learn how to use CloudFront not only for speed improvement but also as a first layer of security (defense in depth). AWS WAF Configuration: Grasp how to write rules to block common types of attacks such as SQL Injection, Cross-Site Scripting (XSS). Architectural Mindset Secure Architecture: Apply a secure Cloud architecture design mindset, ensuring resilience against attacks. Real-world Experience: Discussions with experts helped gain deeper insight into deployment scenarios and practical security challenges. Application to Work Current Project Security: Immediately apply WAF knowledge to configure protection for the Load Balancer (ALB) in the Social Media project. Frontend Optimization: Utilize CloudFront as a mandatory part of the Frontend architecture to increase access speed and ensure HTTPS. Detailed Security Group Configuration: Configure the Security Groups of resources (ALB, ECS, RDS) according to the \u0026ldquo;Least Privilege\u0026rdquo; principle to prevent unwanted access. Event Experience Practical Workshop: The Hands-on Lab sessions provided deeper practical experience compared to theory, boosting my confidence in deploying complex security services. In-depth Discussion: Opportunity to discuss security issues with speakers who have 15-25 years of experience in IT and Security. Quiz Reward: Participating in the trivia game helped reinforce knowledge in a fun way, along with receiving souvenir gifts from AWS. This Workshop is a valuable resource for building a Cloud architecture that not only performs well but is also resilient against modern network threats.\n"},{"uri":"https://khongphuongpro0.github.io/workshopSocial/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Social Media Platform Integrated AWS Cloud Solution for Real-time Social Network 1. Executive Summary The Social Media Platform is designed to provide a comprehensive social communication solution. The platform supports up to 100 concurrent users, scalable to 500 users, using a .NET 8 backend combined with SignalR WebSocket for real-time communication. The platform leverages AWS Cloud services to deliver real-time chat, full social media features (post, comment, reaction, share, story, group, fanpage), and high security with JWT authentication.\n2. Problem Statement Current Problem Existing social media platforms like Facebook and Instagram are overly complex with many unnecessary features, have privacy issues, and high costs. There is no dedicated platform for small communities with customization capabilities and full data ownership.\nSolution The platform uses Amazon ECS with Fargate to run .NET 8 backend containers, Application Load Balancer to distribute traffic (supports WebSocket), Amazon RDS SQL Server for data storage (with AWS DMS for migration), Amazon S3 combined with CloudFront CDN to host and distribute the React frontend and media files. VPC with Subnets and Security Groups ensures network security. SignalR WebSocket provides real-time chat, JWT ensures authentication/authorization, and SendGrid sends confirmation emails. CloudWatch monitors the system. Similar to Facebook and Instagram, users can post, real-time chat (1-1 and group), comment, react, share, create stories (auto-delete after 24h), manage groups and fanpages, add friends, follow and block users. Key features include full chat (history, stickers, reactions, edit/delete messages), comprehensive social features, and optimized operating costs.\nBenefits and Return on Investment (ROI) The solution creates a private social media platform with full data control and high customization, while providing data sources for AI/ML research (sentiment analysis, recommendation systems). The platform replaces expensive enterprise solutions through self-managed systems, simplifies internal communication, and improves engagement. Estimated monthly cost is $90-120 USD for 100 users (self-estimated, not using AWS calculation tools), totaling $1,080-1,440 USD for 12 months. Payback period of 3-6 months through license cost savings and monetization capabilities.\n3. Solution Architecture The platform applies Client-Server architecture with AWS Cloud Infrastructure to manage 100 concurrent users, scalable to 500. React frontend is hosted on S3 and distributed via CloudFront CDN. Traffic goes through Application Load Balancer (supports HTTP/HTTPS and WebSocket) to ECS Fargate containers running .NET 8 backend with SignalR. Primary database is RDS SQL Server (Single-AZ for cost savings), media files stored in S3 (distributed via CloudFront). VPC with Public/Private Subnets and Security Groups ensures network isolation. AWS DMS migrates database to cloud, CloudWatch monitors.\nAWS Services Used\nAmazon ECS + Fargate: Run .NET 8 backend containers, auto-scaling Amazon ECR: Store Docker images Application Load Balancer: Distribute traffic, support WebSocket for SignalR Amazon RDS (SQL Server): Primary database, Single-AZ deployment AWS DMS: Migrate database from on-premise to RDS Amazon S3: Store frontend (bucket 1) and media files (bucket 2) Amazon CloudFront: CDN for frontend and media distribution Amazon VPC: Network isolation with public/private subnets Security Groups: Firewall rules for ALB, ECS, RDS AWS Route 53: DNS management (optional) Amazon CloudWatch: Logs, metrics, alarms Component Design\nFrontend: React app hosted on S3, distributed via CloudFront, accesses backend via HTTPS/WebSocket Load Balancer: ALB routes traffic to ECS tasks, sticky sessions for SignalR Backend Services: ECS Fargate runs 4 tasks (2 REST API + 2 SignalR Hub) Database: RDS SQL Server stores users, posts, messages, stories, groups, pages, relationships Storage: S3 stores avatars, post media, story media; CloudFront cache and distribution Authentication: JWT tokens, SendGrid sends verification emails Monitoring: CloudWatch collects logs and metrics from ECS, ALB, RDS 4. Technical Implementation Implementation Phases The project consists of 2 parts — backend/frontend development and AWS infrastructure deployment — across 4 phases:\nPlanning and Architecture Design: Research .NET 8, SignalR, React, AWS services and design database schema, system architecture (Month 1, Week 1-2) Core Features Development and Continuing AWS Architecture Research: Code authentication, user management, post system, story feature (Month 1 Week 3 - Month 2) Real-time Features Development: Implement SignalR chat (1-1 and group), notifications (Month 2-3) Optimization, Deployment: Container packaging, AWS deployment with CDK/CloudFormation (Month 3) Technical Requirements\nBackend: .NET 8 with SignalR (WebSocket), Entity Framework Core, JWT authentication. Package application with Docker, push images to ECR, deploy to ECS Fargate with 4 tasks (2 API + 2 SignalR). Frontend: React with SignalR Client, Tailwind CSS. Build static files deploy to S3, distribute through CloudFront service. Database: SQL Server with schema: Users, Posts, Comments, Reactions, Messages, Conversations, GroupChats, Stories, Groups, Pages, Friendships, Follows. Optimize by adding Indexes for related tables. Use AWS DMS to migrate from local to RDS. Infrastructure: Use AWS CDK/CloudFormation to configure VPC, Subnets, Security Groups, ALB, ECS Cluster, RDS, S3, CloudFront. Build CI/CD pipeline with GitHub Actions: build → test → push to ECR → update ECS service → deploy frontend to S3. 5. Roadmap \u0026amp; Implementation Milestones Pre-internship (Month 0): Self-learn .NET, basic React research Internship (Month 1–3): Month 1: Week 1-2: Architecture design, database design, AWS setup Week 3-4: User authentication, user APIs Month 2: Week 5-6: Post system Week 7-8: Groups, Pages Week 9-10: 1-1 Chat with SignalR Month 3: Week 11: Group chat, Notifications Week 12: Testing, container packaging, AWS deployment, production launch Post-deployment: Monitoring, maintenance, feature upgrades over 6-12 months 6. Budget Estimation AWS Infrastructure Costs (for 100 users)\nAmazon ECS Fargate: ~80 USD/month (4 tasks: 2 API + 2 SignalR, equivalent to t3.small per task) Application Load Balancer: ~20 USD/month Amazon RDS SQL Server: ~30 USD/month (db.t3.micro, 20GB, Single-AZ) Amazon S3: ~2 USD/month (store 50GB) CloudFront: ~5 USD/month (transfer 100GB data) Data Transfer: ~10 USD/month (outbound data) Amazon ECR: ~1 USD/month CloudWatch: ~5 USD/month (more logs due to 4 tasks) AWS Secrets Manager: ~1 USD/month Route 53: ~1 USD/month (optional) Total: ~155 USD/month, 1,860 USD/12 months\nOptimized with AWS Free Tier: ~90-120 USD/month, 1,080-1,440 USD/12 months\nDevelopment Costs\nSoftware: 0 USD (Visual Studio Community, VS Code, Git - free) Domain: ~12 USD/year SendGrid: 0 USD (free tier: 100 emails/day) Total: ~12 USD/year for domain\n7. Risk Assessment Risk Matrix\nHigh user load: Medium impact, low probability Database failure: High impact, low probability Security breach: High impact, medium probability Budget overrun: Low impact, low probability Mitigation Strategies\nUser load: Auto-scale ECS when needed, ALB load balancing, optimize database indexes Database: Automatic daily backups, point-in-time recovery, can upgrade to Multi-AZ if needed Security: AWS WAF, JWT authentication, input sanitization, HTTPS/WSS encryption, Security Groups Costs: CloudWatch billing alerts ($100/month threshold), turn off unused resources Contingency Plan\nRestore from RDS snapshot if database fails Scale down instances if budget exceeded Use CloudFormation rollback if deployment issues 8. Expected Outcomes Technical Improvements: Real-time chat with response time \u0026lt; 100ms. API response time \u0026lt; 300ms. Support 100 concurrent users, scalable to 500. Uptime 99%.\nLong-term Value: 6+ months data platform for AI/ML research (sentiment analysis, recommendation systems). Full-stack development experience with modern technologies. AWS cloud expertise and DevOps practices. Reusable/extendable for future projects (video call, live streaming, e-commerce).\n"},{"uri":"https://khongphuongpro0.github.io/workshopSocial/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Add role promotion and removal functions in group conversations Add the following functions: Post-related functions (supplementing from previous week) Group creation, dedicated fanpage creation on social media Posting in groups and fanpages Role-related functions like promotion and removal in groups Notification feature Retrieve personal profile pages of users and others Tasks to be carried out this week: Day Tasks Start Date End Date Resources 2 Practice: + Supplement and complete Service and Controller related to conversations, test with PostMan. 11/03/2025 11/03/2025 3 - 4 Practice: + Continue implementing Service and Controller classes for group creation, fanpage creation, post editing, privacy settings adjustment, post deletion, sharing 11/04/2025 11/05/2025 5 Practice: - Create Service and Controller classes related to notification features: + Notify users of successful friend requests + Notify users of post likes + Notify users of post comments + Notify users about group joining, leaving, or removal from groups + Comment reply notifications 11/06/2025 11/06/2025 6 Practice: - Implement Services for remaining functions: + Role promotion and removal in groups + Retrieve personal profile pages of users and others 11/07/2025 11/09/2025 Week 10 Achievements: Completed Services, Controllers and conducted API testing: Page creation, page-related and group-related functions Added role functions in group conversations Notification functions (Controller for PostMan testing not yet completed) Personal profiles, user\u0026rsquo;s personal post lists Post creation "},{"uri":"https://khongphuongpro0.github.io/workshopSocial/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Complete all remaining social media-related features such as: Edit personal information, pages, groups Add page follow-related functions Add post liking, post commenting features Complete Notification feature Controller for testing Implement SignalR to prepare for real-time chat Begin ReactJS implementation to create frontend Tasks to be carried out this week: Day Tasks Start Date End Date Resources 2 Practice: - Complete Services and Controllers, test with PostMan for features: + Edit personal information, groups, pages + Add page follow features 11/10/2025 11/10/2025 3 - 4 Practice: - Continue building Services and Controllers for: + Post liking and commenting features + Complete Controller for testing Notification feature with PostMan 11/11/2025 11/12/2025 5 - Learn about WebSocket and SignalR to prepare for real-time messaging feature Practice: + Implement SignalR for real-time messaging functionality 11/13/2025 11/13/2025 6 - Learn about ReactJS, design architecture to begin frontend development Practice: + Use ReactJS and Bootstrap to build basic pages like login, registration, password recovery, user information, messaging page, personal profile, homepage for viewing posts 11/14/2025 11/16/2025 Week 11 Achievements: Completed Service and Controller classes for all functions Implemented SignalR for real-time messaging Used ReactJS to build framework for main pages Used Bootstrap to create attractive interfaces for all pages "},{"uri":"https://khongphuongpro0.github.io/workshopSocial/1-worklog/1.12-week12/","title":"Worklog Week 12","tags":[],"description":"","content":"Week 12 Goals: Frontend Finalization: Build and integrate completed backend features into the ReactJS interface. Cloud Deployment (AWS): Apply AWS services to deploy the application, ensuring scalability and performance: Deploy Frontend to AWS (S3, CloudFront/Route 53). Deploy Backend using containers (Docker, ECR, ECS). Set up managed Database (RDS). Tasks to be Implemented This Week: Day Task Start Date Completion Date Reference Mon - Tue Frontend Integration (ReactJS): - Connect completed APIs (Service/Controller) (e.g., login, registration, fetching personal info, fetching posts) to the scaffolded ReactJS pages. - Build the complete Home/Feed interface, displaying posts and Like/Comment functionality. 17/11/2025 18/11/2025 Wed Database Deployment (AWS RDS): - Set up the AWS RDS service (e.g., PostgreSQL/MySQL) and configure it to replace the local database. - Update the Connection String in the Backend to point to RDS. [Image of AWS RDS database architecture] | 19/11/2025 | 19/11/2025 | | | Thu | Containerization (Docker \u0026amp; ECR): - Create the Dockerfile for the Backend application. - Build the Docker Image and push it to AWS ECR (Elastic Container Registry) in preparation for deployment.\n[Image of Docker containerization workflow] | 20/11/2025 | 20/11/2025 | | | Fri | Backend Deployment (AWS ECS): - Configure and deploy the AWS ECS (Elastic Container Service) to run the Backend Docker Image from ECR. - Set up Load Balancer and Security Group for ECS.\n[Image of AWS ECS deployment architecture] | 21/11/2025 | 21/11/2025 | | | Sat - Sun | Frontend Deployment (S3, CloudFront \u0026amp; Route 53): - Build the ReactJS application and upload it to AWS S3 (Web Hosting). - Set up AWS CloudFront (CDN) to distribute static content and improve access speed. - Configure AWS Route 53 to point the domain name to CloudFront. | 22/11/2025 | 23/11/2025 | |\nWeek 12 Achieved Results: Completed Frontend: Successfully integrated basic CRUD and interactive features (Like, Comment, Chat) into the ReactJS/Bootstrap interface. Containerized Backend: The Backend application is packaged with Docker and ready for deployment in a container environment. AWS Deployment: Database has been migrated to AWS RDS. Backend has been deployed to AWS ECS. Frontend has been deployed to AWS S3/CloudFront and configured with Route 53. "},{"uri":"https://khongphuongpro0.github.io/workshopSocial/5-workshop/5.4-s3-onprem/","title":"Create NAT Gateway and Configure Route Tables","tags":[],"description":"","content":"In this step, we will create a NAT Gateway to allow Private Subnets to access the internet (e.g., to pull packages from ECR) without being directly accessible from the outside. We will then configure the corresponding Route Tables.\n4.1. Create NAT Gateway Go to the VPC service, NAT Gateways tab, and click Create NAT Gateway. Set a name, choose the mode as Zonal, select a created Public Subnet (e.g., Public-A), select an Elastic IP (create a new one if necessary), and click Create NAT Gateway. Creation is successful. 4.2. Create and Configure Route Tables We will create 3 Route Tables corresponding to 3 types of Subnets: Private-RDS, Private-ECS, and Public.\nGo to the Route Tables tab and select Create route table. Create 3 Route Tables consecutively: Route-RDS, Route-ECS, Route-Public. 4.2.1. Configure Route Table for RDS Subnets (Private) Select Route-RDS, switch to the Subnet Associations tab, and select Edit subnet association. Select the RDS Subnets (both A and B) and click Save. Note: RDS Subnets do not require internet access, so no special Route needs to be added. 4.2.2. Configure Route Table for ECS Subnets (Private) Similarly, associate the ECS Subnets (both A and B) with Route-ECS. Configure a Route that allows internet access via the NAT Gateway: Go to the Routes tab, and select Edit routes. Add a Route: Destination is 0.0.0.0/0, Target is the created NAT Gateway. 4.2.3. Configure Route Table for Public Subnets Associate the Public Subnets (both A and B) with Route-Public. Configure a Route that allows internet access via the Internet Gateway (IGW): Go to the Routes tab, and select Edit routes. Add a Route: Destination is 0.0.0.0/0, Target is the created Internet Gateway. Add the Public Subnets to the Route Table. "},{"uri":"https://khongphuongpro0.github.io/workshopSocial/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section lists and introduces the in-depth technical blogs that I have researched and translated. These articles focus on topics such as SAP on AWS, Hybrid Cloud Architecture, and AI/ML Applications in Software Development, demonstrating an understanding and application of large-scale enterprise solutions.\nBlog 1 - Extending SAP FSM with AWS: A Clean Core Approach This blog introduces a Clean Core Extensibility solution for handling SAP Field Service Management (SAP FSM) data. The article describes a Serverless architecture (using API Gateway, Lambda, EventBridge, and S3) to store SAP FSM attachments in Amazon S3, which helps reduce data volume within SAP FSM and increases data control. This solution also unlocks the potential for integrating AI/ML services like Amazon Textract and Rekognition to process and analyze attachment content.\nBlog 2 - Building Enterprise-Ready Hybrid Network Connectivity for SAP Cloud ERP Private This blog presents three detailed network architecture options (Direct Connect, Hybrid DX + VPN Failover, Landing Zone) for establishing secure, reliable connectivity between the corporate network (on-premises) and the SAP Cloud ERP Private environment (formerly RISE with SAP) on AWS. The article analyzes factors such as performance, cost, latency, and fault tolerance, while guiding the use of services like AWS Direct Connect, Site-to-Site VPN, and AWS Transit Gateway to build an enterprise-grade network foundation.\nBlog 3 - Accelerating SAP Application Development with Amazon Q Developer This blog explores how to use Amazon Q Developer, a Generative AI tool, to boost productivity for SAP developers. The article highlights the challenges of legacy ABAP code and explains how Q Developer can help with automated documentation generation, simplifying the SAP modernization process (transition to S/4HANA and the Clean Core strategy). The tool supports the entire SAP ecosystem (ABAP, RAP, CAP) and has been reported to help increase developer productivity by up to 40%.\n"},{"uri":"https://khongphuongpro0.github.io/workshopSocial/5-workshop/5.5-policy/","title":"Set up Subnet Group and RDS Database","tags":[],"description":"","content":"In this step, we will create a DB Subnet Group (mandatory for RDS) and then initialize Amazon RDS (Relational Database Service) for the backend database.\n5.1. Create DB Subnet Group In the RDS service, go to the Subnet Groups tab, and click Create DB Subnet Group. Set a name (e.g., social-media-db-subnet-group), and select the created VPC. Select the 2 corresponding AZs and the 2 Private Subnets created for RDS (Private-RDS-A and Private-RDS-B). Subnet Group creation is successful. 5.2. Create RDS Database (MySQL) Go to the RDS service and click Create database. Select the MySQL Engine. Choose the version and select the Free tier Template to save costs. Set the Database name, the account name (Master username), and the password (Master password). Select the DB instance class as t4g.micro (Free tier). Select Storage as gp3. Select the VPC and the Subnet Group created above. In the Public access section, select No (since this is a Private Subnet). Then click Create database. Creation is successful. 5.2.1. Temporarily Modify Public Access and Migration To perform the initial database migration, we need to temporarily change Public access to Yes (DMS can be used for larger databases). Modification is successful. Perform Database Migration/Import: You can use MySQL Workbench or the Command Line. Verify by connecting MySQL Workbench to RDS. The database contains data. "},{"uri":"https://khongphuongpro0.github.io/workshopSocial/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in 2 important events. Each event was a memorable experience, providing new, interesting, and valuable knowledge, along with great moments and gifts. Participating in these workshops/events helped me stay updated on the latest technology trends and apply them directly to my internship project.\n🚀 Event 1: OJT Kick-off Ceremony Event Name: RECAP | KICK-OFF AWS FIRST CLOUD JOURNEY WORKFORCE – OJT FALL 2025\nDate: September 6, 2025\nLocation: Bitexco Financial Tower, No. 02 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City.\nRole in Event: Participant\nBrief Description:\nThe event officially marked the beginning of the AWS First Cloud Journey Workforce – OJT FALL 2025 program, an intensive training journey in Cloud Computing. The main content included:\nKeynote \u0026amp; Industry Sharing: AWS Vietnam specialists shared directions on Cloud, AI, and DevOps trends. Alumni \u0026amp; Career Sharing: Interaction with successful alumni, who are currently Cloud Engineers and GenAI Engineers. Networking \u0026amp; Q\u0026amp;A: Opportunity to connect with experts and the tech community. Value Gained:\nClarity of Vision: Gained a clear understanding of the program\u0026rsquo;s objectives and the future trajectory of Cloud and AI/ML careers. Networking: Established initial connections with peers and industry experts. 🛡️ Event 2: Application Protection Workshop on AWS Event Name: Secure Your Applications: AWS Perimeter Protection Workshop | Ho Chi Minh City\nTime: 09:00 - 17:00 on November 19, 2025\nLocation: 26th Floor (Room 26.100), Bitexco Financial Tower, No. 02 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City.\nRole in Event: Participant\nBrief Description:\nAn in-depth, hands-on workshop focused on solutions for protecting web applications from threats like DDoS and Application Layer Attacks using AWS edge security services. Key activities included:\nArchitecture \u0026amp; Optimization: Learning about Amazon CloudFront as a content delivery platform and its security controls. Defense \u0026amp; WAF: Protecting applications using AWS WAF to mitigate OWASP Top 10 vulnerabilities. Hands-on Labs: Practical exercises on optimizing and securing internet-facing web applications, guided by AWS Solution Architects. A Quiz activity was held at the end for prizes. Value Gained:\nSecurity Skills: Acquired practical skills in configuring AWS WAF and CloudFront to implement perimeter defense. Direct Application: The knowledge is directly applicable to securing the Social Media project, enhancing my understanding of robust Cloud security architecture (a crucial skill for Cloud/DevOps). Technical Networking: Collaborated with other participants during the hands-on lab sessions. "},{"uri":"https://khongphuongpro0.github.io/workshopSocial/5-workshop/5.6-cleanup/","title":"ECR, Docker, Security Group, ALB, ECS Service, and Conclusion","tags":[],"description":"","content":"These are the final steps to package the Backend application, create the security/Load Balancer components, and deploy it to ECS Fargate.\n6.1. Docker, ECR, and Push Image 6.1.1. Create ECR (Elastic Container Registry) Go to the ECR service and create a Repository (e.g., social-media-backend). ECR is now available. 6.1.2. Dockerfile and Build Image Create the Dockerfile parallel to the backend folder (using the Onion architecture structure). Configure the Docker file. Run the command docker build -t social-media-backend . to build. Packaging is successful. 6.1.3. Log in and Push Image to ECR Log in to ECR using the command line, verify the tag, and check again with docker image. Run the command to push the image to ECR. Push is complete. 6.2. Create Security Groups We create 3 Security Groups: 1 for ALB, 1 for ECS/Backend, and 1 for RDS/Database.\n6.2.1. Security Group for ALB (Internet-Facing) Select Create security group. Set a name, and select the created VPC. In the Inbound rules section, select HTTP and HTTPS with Source as Anywhere (0.0.0.0/0 and ::/0), click Create. 6.2.2. Security Group for ECS (Backend) Set a name, and select the VPC. Note: The Dockerfile\u0026rsquo;s Port is 8080, so the Inbound rule will be Custom TCP Port 8080. The Source will be the Security Group of the ALB just created (only the ALB is allowed to send traffic to ECS). 6.2.3. Security Group for RDS (Database) Set a name, and select the VPC. Note: The Inbound rule will be MySQL Port (3306). The Source will be the Security Group of the ECS just created (only ECS is allowed to access RDS). In summary, we have 3 Security Groups. 6.3. Create Target Group and ALB 6.3.1. Create Target Group Go to EC2 -\u0026gt; Target Group, select Create target group. Select Type as IP address. Set the group name, Port as 8080 (configured in the Dockerfile), and select the created VPC. In the Healthy threshold section, select 2, leave the rest as default, and click Create. Target Group creation is complete. 6.3.2. Create Application Load Balancer (ALB) In EC2, switch to Load Balancers, click Create Load Balancer. Select Application Load Balancer. Set a name, select Scheme as internet-facing, IP address type as Ipv4. In Network mapping, select 2 AZs and 2 corresponding Public Subnets. In Security group, select the Security Group created for the ALB. In Listeners, we create 2 Listeners: Listener 1 (HTTP 80): Configure to redirect to HTTPS. * **Listener 2 (HTTPS 443)**: In **Routing action**, select **Forward to target group**, choose the Target Group just created. In the secure configuration section for the Listener (HTTPS), select the Certification created in the Route 53 section (ACM). Click Create. The screen shows successful creation. 6.4. Create ECS Cluster and Service 6.4.1. Create ECS Cluster Go to the ECS service, Clusters tab, click Create Cluster. Set a name and choose the architecture as Fargate only. Click Create. Creation is successful. 6.4.2. Configure Task Definition Go to the Task definitions tab, click Create new task definition. Choose to create using a JSON file (top right corner of the screen). Use the JSON file structure containing information such as: ECR image, corresponding port, environment variables (database connection), and CORS (allowing the purchased domain address). Click Create. Creation is successful. 6.4.3. Create ECS Service Go to the Cluster just created, scroll down to the Services tab, and select Create Service. In Service detail, select the newly created Task, set the Service name. In Environment, select Launch type (Fargate). In Deployment configuration, leave as default. In Networking, select the created VPC, the 2 Private Subnets corresponding to ECS, and disable the Public IP function. In the Load balancing section, check Use load balancing. Use the existing ALB and the Listener created earlier. Select the existing Target Group and choose the Target Group we created in the previous step. Finally, click Create. The Task has run successfully. 6.4.4. Configure Route 53 Record pointing to ALB Create a Route 53 record, configured to point to the created ALB. The Name is api (e.g., api.social-sgu-media.click), and click Create. Creation is successful. 6.5. Result Test the configured API URL (e.g., https://api.social-sgu-media.click/).\nSuccess! Thus, we have completed the Workshop for deploying the Social Media application on AWS.\n"},{"uri":"https://khongphuongpro0.github.io/workshopSocial/5-workshop/","title":"Workshop: Deploying a Social Media Web Application on AWS","tags":[],"description":"","content":"🛠️ Workshop: Deploying a Social Media Web Application on AWS This Workshop provides detailed, step-by-step guidance for deploying a Social Media web application (including both Frontend and Backend) onto the Amazon Web Services (AWS) platform. We will build a modern, secure, and scalable architecture using Cloud services such as VPC, S3, CloudFront, RDS, ECR, ALB, and ECS Fargate.\n[Image of Three-Tier Web Application Architecture on AWS]\nMain Objectives Establish a secure and highly available network environment (VPC). Deploy the static Frontend on S3/CloudFront (using HTTPS and Route 53). Package and deploy the Backend as a container on ECS Fargate. Utilize RDS for the data tier (Database). Workshop Steps 5.1. Workshop Overview An overview of the objectives, architecture, and expected outcomes of the Workshop.\n5.2. Prerequisites Preparation of necessary AWS accounts, tools (AWS CLI, Docker), and required source code before starting.\n5.3. S3, CloudFront, and VPC Setup 5.3.1. Create S3 for Upload and Frontend: Create S3 buckets for file uploads and Frontend hosting, and configure IAM for uploading. 5.3.2. Configure CDN and Route 53: Set up CloudFront (CDN) to serve the Frontend over HTTPS and configure the domain via Route 53 and ACM. 5.3.3. Set up VPC Network: Create the VPC, Internet Gateway, and necessary Subnets (Public/Private). 5.4. Advanced Network Configuration Create NAT Gateway and Route Table: Set up the NAT Gateway for Private Subnets to access the Internet (outbound) and configure corresponding Route Tables for different Subnet types. 5.5. RDS Database Setup Create DB Subnet Group and RDS: Initialize the DB Subnet Group and the MySQL/RDS Database within a Private Subnet. 5.6. Container Deployment and Load Balancing (Cleanup) Docker, ECR, and Push Image: Package the Backend with Docker and push the image to ECR. Security Group and ALB: Create Security Groups and set up the Application Load Balancer (ALB) to securely route traffic to the Backend. ECS Cluster and Service: Configure the Task Definition, create the ECS Cluster (Fargate), and the ECS Service to deploy the Backend. Conclusion: Final Route 53 configuration to point the API domain to the ALB and verify the results. "},{"uri":"https://khongphuongpro0.github.io/workshopSocial/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"Throughout my internship at [Company/Organization Name] in the FCJ (First Cloud Journey) program from [start date] to [end date], I had the opportunity to learn, train, and apply knowledge of Cloud Computing, DevOps, and Programming in a real-world work environment. I participated in building the Cloud architecture and deploying the Social Media application onto the AWS platform (using ECS Fargate, ALB, RDS, VPC, etc.), thereby improving my skills in architecture analysis, automation (DevOps), system error handling, and in-depth research on Cloud services.\nRegarding work attitude, I always strove to complete tasks and actively interact with colleagues. However, I recognize that there are still many shortcomings that need improvement.\nTo objectively reflect on my internship process, I would like to self-assess based on the criteria below:\nNo. Criteria Description Excellent Good Average 1 Professional Knowledge and Skills Industry knowledge, practical application of knowledge, tool usage skills, work quality ✅ ☐ ☐ 2 Learning Ability Acquiring new knowledge, fast learning ☐ ✅ ☐ 3 Proactiveness Self-learning, taking on tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of Responsibility Completing work on time, ensuring quality ☐ ✅ ☐ 5 Discipline Adhering to time schedules, regulations, and work procedures ☐ ☐ ✅ 6 Drive for Improvement Readiness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Clear presentation of ideas, coherent work reporting ☐ ✅ ☐ 8 Teamwork Effective collaboration with colleagues, active participation ✅ ☐ ☐ 9 Professional Conduct Respect for colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-Solving Mindset Issue identification, solution proposal, creativity ☐ ✅ ☐ 11 Contribution to Project/Organization Work effectiveness, improvement initiatives, recognition from the team ☐ ✅ ☐ 12 Overall Assessment General assessment of the entire internship process ☐ ✅ ☐ Areas for Improvement and Lessons Learned Regarding Discipline and Time Management: I realize that I need to seriously improve my discipline during working hours. I sometimes failed to focus completely, leading to tasks being pushed to the final phase. The lesson learned is that adhering to schedules and regulations is not only a rule but also directly affects work efficiency and quality.\nRegarding Responsibility and Task Completion: Despite my best efforts, I was unable to complete the final Deployment step of the Social Media project onto AWS due to complex technical errors that arose near the deadline. This shows that I lacked adequate contingency planning and risk management. The lesson learned is not to leave crucial work until the last minute; sufficient time must be allocated for testing, error handling, and I must report issues early when I encounter intractable problems.\nRegarding Problem-Solving Skills: My ability to handle unexpected and complex errors, especially in a Cloud environment, needs improvement. I spent too much time trying to resolve an error independently instead of seeking timely assistance. \u0026gt; \u0026gt; Improvement: Enhance Problem-Solving Mindset, especially the ability to perform Root Cause Analysis, and learn how to seek support from mentors or colleagues more effectively.\nRegarding Communication: \u0026gt; I need to learn to communicate more clearly and concisely in work and difficult situations, especially when reporting progress or emerging issues (e.g., deployment errors). I have recognized these weaknesses and consider them invaluable experience for changing my work style and methodology in the future.\n"},{"uri":"https://khongphuongpro0.github.io/workshopSocial/7-feedback/","title":"Feedback and Suggestions","tags":[],"description":"","content":" This section contains my personal feedback on the First Cloud Journey (FCJ) program. These comments are intended to help the FCJ Team and the company improve the experience for subsequent generations of interns.\nGeneral Assessment of the FCJ Program Category Assessment \u0026amp; Suggestions for Improvement 1. Work Environment The work environment is friendly and open, creating excellent conditions for learning. The workspace is comfortable, which helps increase focus. Suggestion: To enhance cohesion, I suggest organizing informal \u0026ldquo;Tech Talks\u0026rdquo; or \u0026ldquo;Coding Challenges\u0026rdquo; sessions interspersed with Team Building activities, which would help interns understand each other better, both professionally and personally. 2. Mentor / Team Admin Support The support provided was excellent. The Mentor not only explained knowledge in detail but also encouraged \u0026ldquo;trial and error\u0026rdquo; (allowing me to \u0026ldquo;fail fast\u0026rdquo;). This was highly valuable. The Team Admin efficiently handled procedures and documentation. Suggestion: Consider providing a more detailed checklist of expected technical risks (e.g., common ECR/ECS errors) early on, or increase short 1-on-1 sessions during the crucial final phase to ensure no issues are missed. 3. Alignment with Major The tasks assigned were highly relevant, especially in the areas of Cloud Architecture and DevOps. The program helped me transform theoretical knowledge of VPC and RDS from textbooks into practical experience on AWS. This is a significant strength of the program. 4. Learning \u0026amp; Skill Development Opportunities Excellent. I learned how to work with project management tools and basic CI/CD pipelines. The most crucial skill gained was the ability to self-research and find official AWS documentation. The professional work environment helped me shape a clearer Career Path. 5. Culture \u0026amp; Team Spirit The culture of respect, positivity, and seriousness was a great motivator. Everyone was willing to support colleagues during urgent tasks. I genuinely felt like a member of the team, not just an intern. 6. Intern Policy / Benefits Support related to stipend, flexible time when needed, and the opportunity to attend internal training sessions were major plusses. Additional Questions What were you most satisfied with during the internship?\nWhat I was most satisfied with was the opportunity to access and personally deploy a complete Cloud architecture (end-to-end) from A-Z. This allowed me to gain practical AWS experience much faster than I could have through theory alone.\nWhat do you think the company needs to improve for future interns?\nThe company should emphasize the importance of work discipline and time management even more starting from the first few weeks. Although reminded, my failure to complete the final Deployment due to a technical error near the deadline taught me a huge lesson about avoiding procrastination and the need to allocate sufficient buffer time for system testing. This could be addressed with regular progress check-ins (e.g., twice a week) even when things seem stable.\nIf recommending to friends, would you advise them to intern here? Why?\nAbsolutely. I would strongly recommend friends interested in Cloud/DevOps to join because this is a rare environment that allows them to:\nGain hands-on experience with Production-grade Cloud architecture. Receive guidance from skillful, dedicated Mentors who encourage self-learning. Suggestions \u0026amp; Future Intent Do you have any suggestions to improve the internship experience?\nAdd formal Technical Review sessions (e.g., every two weeks) where interns present the architecture they have built, helping them train in professional Communication and Technical Presentation skills. Add a dedicated session on \u0026ldquo;Debugging and Root Cause Analysis (RCA)\u0026rdquo; in a Cloud environment, as this is a key skill for deployment execution. Would you like to continue this program in the future?\nYes, I am very eager to continue this program until the end of the season, not just the internship period, or have the opportunity to become a permanent employee, to continue contributing and learning more about advanced Cloud services.\nOther Feedback (feel free to share):\nI would like to express my sincere thanks to the Mentor and the FCJ Team. Despite some setbacks (like the final Deployment error), those very failures provided me with invaluable practical lessons on discipline and preparation that no classroom could teach.\n"},{"uri":"https://khongphuongpro0.github.io/workshopSocial/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://khongphuongpro0.github.io/workshopSocial/categories/sap-on-aws/","title":"SAP on AWS","tags":[],"description":"","content":""},{"uri":"https://khongphuongpro0.github.io/workshopSocial/categories/serverless/","title":"Serverless","tags":[],"description":"","content":""},{"uri":"https://khongphuongpro0.github.io/workshopSocial/categories/amazon-q/","title":"Amazon Q","tags":[],"description":"","content":""},{"uri":"https://khongphuongpro0.github.io/workshopSocial/categories/amazon-q-developer/","title":"Amazon Q Developer","tags":[],"description":"","content":""},{"uri":"https://khongphuongpro0.github.io/workshopSocial/categories/announcements/","title":"Announcements","tags":[],"description":"","content":""},{"uri":"https://khongphuongpro0.github.io/workshopSocial/categories/aws-direct-connect/","title":"AWS Direct Connect","tags":[],"description":"","content":""},{"uri":"https://khongphuongpro0.github.io/workshopSocial/categories/aws-site-to-site-vpn/","title":"AWS Site-to-Site VPN","tags":[],"description":"","content":""},{"uri":"https://khongphuongpro0.github.io/workshopSocial/categories/aws-transit-gateway/","title":"AWS Transit Gateway","tags":[],"description":"","content":""},{"uri":"https://khongphuongpro0.github.io/workshopSocial/categories/best-practices/","title":"Best Practices","tags":[],"description":"","content":""},{"uri":"https://khongphuongpro0.github.io/workshopSocial/tags/","title":"Tags","tags":[],"description":"","content":""}]